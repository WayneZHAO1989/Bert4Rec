{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46916904-dd4e-4a77-86bd-b53d68f1f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import argparse\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from abc import *\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54adaa20-07ac-43b5-9d7b-181fbc06035f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e72911-d946-47ec-8ef6-a1b740a32e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c82bd412-319e-4107-9bc8-6ec649275b60",
   "metadata": {},
   "source": [
    "## arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bccaf173-9670-40dd-ac86-dd6ac61f4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('--data_path', type=str, default='./data/ml-1m/ratings.dat')\n",
    "parser.add_argument('--min_rating', type=int, default=0, help='Only keep ratings greater than equal to this value')\n",
    "parser.add_argument('--min_uc', type=int, default=5, help='Only keep users with more than min_uc ratings')\n",
    "parser.add_argument('--min_sc', type=int, default=0, help='Only keep items with more than min_sc ratings')\n",
    "\n",
    "parser.add_argument('--dataset_split_seed', type=int, default=98765)\n",
    "parser.add_argument('--eval_set_size', type=int, default=500,  help='Size of val and test set. 500 for ML-1m and 10000 for ML-20m recommended')\n",
    "\n",
    "# Dataloader\n",
    "parser.add_argument('--dataloader_random_seed', type=float, default=12345)\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "\n",
    "# NegativeSampler\n",
    "parser.add_argument('--train_negative_sampler_code', type=str, default='random', help='Method to sample negative items for training. Not used in bert')\n",
    "parser.add_argument('--train_negative_sample_size', type=int, default=0)\n",
    "parser.add_argument('--train_negative_sampling_seed', type=int, default=12345)\n",
    "parser.add_argument('--test_negative_sampler_code', type=str, default='random',help='Method to sample negative items for evaluation')\n",
    "parser.add_argument('--test_negative_sample_size', type=int, default=100)\n",
    "parser.add_argument('--test_negative_sampling_seed', type=int, default=12345)\n",
    "\n",
    "\n",
    "# device #\n",
    "parser.add_argument('--device', type=str, default='cpu', choices=['cpu', 'cuda'])\n",
    "parser.add_argument('--num_gpu', type=int, default=0)\n",
    "parser.add_argument('--device_idx', type=str, default='-1')\n",
    "# optimizer #\n",
    "parser.add_argument('--optimizer', type=str, default='Adam', choices=['SGD', 'Adam'])\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0, help='l2 regularization')\n",
    "\n",
    "# lr scheduler #\n",
    "parser.add_argument('--decay_step', type=int, default=15, help='Decay step for StepLR')\n",
    "parser.add_argument('--gamma', type=float, default=0.1, help='Gamma for StepLR')\n",
    "# epochs #\n",
    "parser.add_argument('--num_epochs', type=int, default=32, help='Number of epochs for training')\n",
    "# logger #\n",
    "parser.add_argument('--log_period_as_iter', type=int, default=12800)\n",
    "# evaluation #\n",
    "parser.add_argument('--metric_ks', nargs='+', type=int, default=[5, 10, 20, 50, 100], help='ks for Metric@k')\n",
    "parser.add_argument('--best_metric', type=str, default='NDCG@10', help='Metric for determining the best model')\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--model_init_seed', type=int, default=None)\n",
    "# BERT #\n",
    "parser.add_argument('--bert_max_len', type=int, default=100, help='Length of sequence for bert')\n",
    "parser.add_argument('--bert_num_items', type=int, default=None, help='Number of total items')\n",
    "parser.add_argument('--bert_hidden_units', type=int, default=256, help='Size of hidden vectors (d_model)')\n",
    "parser.add_argument('--bert_num_blocks', type=int, default=2, help='Number of transformer layers')\n",
    "parser.add_argument('--bert_num_heads', type=int, default=4, help='Number of heads for multi-attention')\n",
    "parser.add_argument('--bert_dropout', type=float, default=0.1, help='Dropout probability to use throughout the model')\n",
    "parser.add_argument('--bert_mask_prob', type=float, default=0.15, help='Probability for masking items in the training sequence')\n",
    "\n",
    "# Experiment\n",
    "parser.add_argument('--experiment_dir', type=str, default='experiments')\n",
    "parser.add_argument('--experiment_description', type=str, default='test')\n",
    "\n",
    "################\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750e3dc-3a3e-45f5-81ff-93d0a2d2fdca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a72debf-deaf-4053-8547-66e4b781e057",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed4c828-d7ff-4363-9c2e-a2c95f48139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(filePath, min_rating=0, min_sc=0, min_uc=5):\n",
    "    \n",
    "    df = pd.read_csv(filePath, sep='::', header=None)\n",
    "    df.columns = ['uid', 'sid', 'rating', 'timestamp']\n",
    "    \n",
    "    # Turning into implicit ratings\n",
    "    df = df[df['rating'] >= min_rating]\n",
    "    \n",
    "    # Filtering triplets\n",
    "    if min_sc > 0:\n",
    "        item_sizes = df.groupby('sid').size()\n",
    "        good_items = item_sizes.index[item_sizes >= min_sc]\n",
    "        df = df[df['sid'].isin(good_items)]\n",
    "    \n",
    "    if min_uc > 0:\n",
    "        user_sizes = df.groupby('uid').size()\n",
    "        good_users = user_sizes.index[user_sizes >= min_uc]\n",
    "        df = df[df['uid'].isin(good_users)]\n",
    "    \n",
    "    # Densifying index\n",
    "    umap = {u: i for i, u in enumerate(set(df['uid']))}\n",
    "    smap = {s: i for i, s in enumerate(set(df['sid']))}\n",
    "    df['uid'] = df['uid'].map(umap)\n",
    "    df['sid'] = df['sid'].map(smap)\n",
    "    \n",
    "    # Split into train/val/test\n",
    "    user_group = df.groupby('uid')\n",
    "    user2items = user_group.apply(lambda d: list(d.sort_values(by='timestamp')['sid']))\n",
    "    train, val, test = {}, {}, {}\n",
    "    for user in range(len(umap)):\n",
    "        items = user2items[user]\n",
    "        train[user], val[user], test[user] = items[:-2], items[-2:-1], items[-1:]\n",
    "    \n",
    "    return df, umap, smap, train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4af858-cbaa-4214-bcb3-036de2193f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNegativeSampler(metaclass=ABCMeta):\n",
    "    def __init__(self, train, val, test, user_count, item_count, sample_size, seed, save_folder):\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.test = test\n",
    "        self.user_count = user_count\n",
    "        self.item_count = item_count\n",
    "        self.sample_size = sample_size\n",
    "        self.seed = seed\n",
    "        self.save_folder = save_folder\n",
    "\n",
    "    def generate_negative_samples(self):\n",
    "        assert self.seed is not None, 'Specify seed for random sampling'\n",
    "        np.random.seed(self.seed)\n",
    "        negative_samples = {}\n",
    "        print('Sampling negative items')\n",
    "        for user in trange(self.user_count):\n",
    "            if isinstance(self.train[user][1], tuple):\n",
    "                seen = set(x[0] for x in self.train[user])\n",
    "                seen.update(x[0] for x in self.val[user])\n",
    "                seen.update(x[0] for x in self.test[user])\n",
    "            else:\n",
    "                seen = set(self.train[user])\n",
    "                seen.update(self.val[user])\n",
    "                seen.update(self.test[user])\n",
    "\n",
    "            samples = []\n",
    "            for _ in range(self.sample_size):\n",
    "                item = np.random.choice(self.item_count) + 1\n",
    "                while item in seen or item in samples:\n",
    "                    item = np.random.choice(self.item_count) + 1\n",
    "                samples.append(item)\n",
    "\n",
    "            negative_samples[user] = samples\n",
    "\n",
    "        return negative_samples\n",
    "\n",
    "    def get_negative_samples(self):\n",
    "        savefile_path = 'random_negative_samplesize{}_seed{}.pkl'.format(self.sample_size, self.seed)\n",
    "        print(\"negative samples path:\", savefile_path)\n",
    "        if os.path.exists(savefile_path):\n",
    "            print('Negatives samples exist. Loading.')\n",
    "            negative_samples = pickle.load(open(savefile_path, 'rb'))\n",
    "        else:\n",
    "            print(\"Negative samples don't exist. Generating.\")\n",
    "            negative_samples = self.generate_negative_samples()\n",
    "            with open(savefile_path, 'wb') as f:\n",
    "                pickle.dump(negative_samples, f)\n",
    "        return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7adae615-2145-455f-b2fd-037c4659ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataloader(metaclass=ABCMeta):\n",
    "    def __init__(self, args, umap, smap, train, val, test, save_folder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.rng = random.Random(12345)\n",
    "        \n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.test = test\n",
    "        self.umap = umap\n",
    "        self.smap = smap\n",
    "        self.user_count = len(self.umap)\n",
    "        self.item_count = len(self.smap)\n",
    "        \n",
    "        self.save_folder = save_folder\n",
    "        self.max_len = args.bert_max_len\n",
    "        self.mask_prob = args.bert_mask_prob\n",
    "        self.CLOZE_MASK_TOKEN = self.item_count + 1\n",
    "\n",
    "        train_negative_sampler = RandomNegativeSampler(self.train, self.val, self.test,\n",
    "                                                          self.user_count, self.item_count,\n",
    "                                                          args.train_negative_sample_size,\n",
    "                                                          args.train_negative_sampling_seed,\n",
    "                                                          self.save_folder)\n",
    "\n",
    "        test_negative_sampler = RandomNegativeSampler(self.train, self.val, self.test,\n",
    "                                                         self.user_count, self.item_count,\n",
    "                                                         args.test_negative_sample_size,\n",
    "                                                         args.test_negative_sampling_seed,\n",
    "                                                         self.save_folder)\n",
    "\n",
    "        self.train_negative_samples = train_negative_sampler.get_negative_samples()\n",
    "        self.test_negative_samples = test_negative_sampler.get_negative_samples()\n",
    "\n",
    "    def get_pytorch_dataloaders(self):\n",
    "        train_loader = self._get_train_loader()\n",
    "        val_loader = self._get_eval_loader(mode='val')\n",
    "        test_loader = self._get_eval_loader(mode='test')\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def _get_train_loader(self):\n",
    "        dataset = BertTrainDataset(self.train, self.max_len, self.mask_prob, self.CLOZE_MASK_TOKEN, self.item_count, self.rng)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.args.batch_size, shuffle=True, pin_memory=True)\n",
    "        return dataloader\n",
    "\n",
    "    def _get_eval_loader(self, mode):\n",
    "        answers = self.val if mode == 'val' else self.test\n",
    "        dataset = BertEvalDataset(self.train, answers, self.max_len, self.CLOZE_MASK_TOKEN, self.test_negative_samples)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.args.batch_size, shuffle=False, pin_memory=True)\n",
    "        return dataloader\n",
    "\n",
    "class BertTrainDataset(Dataset):\n",
    "    def __init__(self, u2seq, max_len, mask_prob, mask_token, item_count, rng):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token = mask_token\n",
    "        self.item_count = item_count\n",
    "        self.rng = rng\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self._getseq(user)\n",
    "\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for s in seq:\n",
    "            prob = self.rng.random()\n",
    "            if prob < self.mask_prob:\n",
    "                prob /= self.mask_prob\n",
    "\n",
    "                if prob < 0.8:\n",
    "                    tokens.append(self.mask_token)\n",
    "                elif prob < 0.9:\n",
    "                    tokens.append(self.rng.randint(1, self.item_count))\n",
    "                else:\n",
    "                    tokens.append(s)\n",
    "\n",
    "                labels.append(s)\n",
    "            else:\n",
    "                tokens.append(s)\n",
    "                labels.append(0)\n",
    "\n",
    "        tokens = tokens[-self.max_len:]\n",
    "        labels = labels[-self.max_len:]\n",
    "\n",
    "        mask_len = self.max_len - len(tokens)\n",
    "\n",
    "        tokens = [0] * mask_len + tokens\n",
    "        labels = [0] * mask_len + labels\n",
    "\n",
    "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
    "\n",
    "    def _getseq(self, user):\n",
    "        return self.u2seq[user]\n",
    "\n",
    "class BertEvalDataset(Dataset):\n",
    "    def __init__(self, u2seq, u2answer, max_len, mask_token, negative_samples):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.u2answer = u2answer\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self.u2seq[user]\n",
    "        answer = self.u2answer[user]\n",
    "        negs = self.negative_samples[user]\n",
    "\n",
    "        candidates = answer + negs\n",
    "        labels = [1] * len(answer) + [0] * len(negs)\n",
    "\n",
    "        seq = seq + [self.mask_token]\n",
    "        seq = seq[-self.max_len:]\n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = [0] * padding_len + seq\n",
    "\n",
    "        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c318c-0e4a-4545-99c2-26feb2830809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ad0c986-2a9d-4c16-bd5d-2e3782a48634",
   "metadata": {},
   "source": [
    "## Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f301944d-141a-4f0a-afc1-447f75ff137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding learnable positional information\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.position_embed = nn.Embedding(max_len, embed_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size = input_seq.size(0)\n",
    "        token_embed = self.token_embed(input_seq) \n",
    "        position_embed = self.position_embed.weight.unsqueeze(0).repeat(batch_size,1,1)\n",
    "        return self.dropout( token_embed + position_embed)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        ## Attention\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        ## FFN\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "class BERTModel(nn.Module, metaclass=ABCMeta):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        # fix_random_seed_as(args.model_init_seed)\n",
    "        # self.init_weights()\n",
    "\n",
    "        max_len = args.bert_max_len\n",
    "        num_items = args.num_items\n",
    "        n_layers = args.bert_num_blocks\n",
    "        heads = args.bert_num_heads\n",
    "        vocab_size = num_items + 2\n",
    "        hidden = args.bert_hidden_units\n",
    "        dropout = args.bert_dropout\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden, max_len=max_len, dropout=dropout)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, heads, hidden * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "        self.out = nn.Linear(hidden, args.num_items + 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "\n",
    "        # output \n",
    "        return self.out(x)\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1cf353-e9f9-4bbd-ac95-91aaf81e4ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b02f8706-6216-4885-9696-3126d94d6e08",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210a7e6d-8cea-4137-a4d7-ad6e7a102a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(batch):\n",
    "    seqs, labels = batch\n",
    "    logits = model(seqs)  # B x T x V\n",
    "    logits = logits.view(-1, logits.size(-1))  # (B*T) x V\n",
    "    labels = labels.view(-1)  # B*T\n",
    "    loss = ce(logits, labels)\n",
    "    return loss\n",
    "\n",
    "def calculate_metrics(batch, metric_ks=args.metric_ks):\n",
    "    seqs, candidates, labels = batch\n",
    "    scores = model(seqs)  # B x T x V\n",
    "    scores = scores[:, -1, :]  # B x V\n",
    "    scores = scores.gather(1, candidates)  # B x C\n",
    "\n",
    "    metrics = recalls_and_ndcgs_for_ks(scores, labels, metric_ks)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "class AverageMeterSet(object):\n",
    "    def __init__(self, meters=None):\n",
    "        self.meters = meters if meters else {}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key not in self.meters:\n",
    "            meter = AverageMeter()\n",
    "            meter.update(0)\n",
    "            return meter\n",
    "        return self.meters[key]\n",
    "\n",
    "    def update(self, name, value, n=1):\n",
    "        if name not in self.meters:\n",
    "            self.meters[name] = AverageMeter()\n",
    "        self.meters[name].update(value, n)\n",
    "\n",
    "    def reset(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.reset()\n",
    "\n",
    "    def values(self, format_string='{}'):\n",
    "        return {format_string.format(name): meter.val for name, meter in self.meters.items()}\n",
    "\n",
    "    def averages(self, format_string='{}'):\n",
    "        return {format_string.format(name): meter.avg for name, meter in self.meters.items()}\n",
    "\n",
    "    def sums(self, format_string='{}'):\n",
    "        return {format_string.format(name): meter.sum for name, meter in self.meters.items()}\n",
    "\n",
    "    def counts(self, format_string='{}'):\n",
    "        return {format_string.format(name): meter.count for name, meter in self.meters.items()}\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __format__(self, format):\n",
    "        return \"{self.val:{format}} ({self.avg:{format}})\".format(self=self, format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e53422c-4427-4515-8fc6-04251b55e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def recall(scores, labels, k):\n",
    "    scores = scores\n",
    "    labels = labels\n",
    "    rank = (-scores).argsort(dim=1)\n",
    "    cut = rank[:, :k]\n",
    "    hit = labels.gather(1, cut)\n",
    "    return (hit.sum(1).float() / torch.min(torch.Tensor([k]).to(hit.device), labels.sum(1).float())).mean().cpu().item()\n",
    "\n",
    "\n",
    "def ndcg(scores, labels, k):\n",
    "    scores = scores.cpu()\n",
    "    labels = labels.cpu()\n",
    "    rank = (-scores).argsort(dim=1)\n",
    "    cut = rank[:, :k]\n",
    "    hits = labels.gather(1, cut)\n",
    "    position = torch.arange(2, 2+k)\n",
    "    weights = 1 / torch.log2(position.float())\n",
    "    dcg = (hits.float() * weights).sum(1)\n",
    "    idcg = torch.Tensor([weights[:min(int(n), k)].sum() for n in labels.sum(1)])\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg.mean()\n",
    "\n",
    "\n",
    "def recalls_and_ndcgs_for_ks(scores, labels, ks):\n",
    "    metrics = {}\n",
    "\n",
    "    scores = scores\n",
    "    labels = labels\n",
    "    answer_count = labels.sum(1)\n",
    "\n",
    "    labels_float = labels.float()\n",
    "    rank = (-scores).argsort(dim=1)\n",
    "    cut = rank\n",
    "    for k in sorted(ks, reverse=True):\n",
    "       cut = cut[:, :k]\n",
    "       hits = labels_float.gather(1, cut)\n",
    "       metrics['Recall@%d' % k] = \\\n",
    "           (hits.sum(1) / torch.min(torch.Tensor([k]).to(labels.device), labels.sum(1).float())).mean().cpu().item()\n",
    "\n",
    "       position = torch.arange(2, 2+k)\n",
    "       weights = 1 / torch.log2(position.float())\n",
    "       dcg = (hits * weights.to(hits.device)).sum(1)\n",
    "       idcg = torch.Tensor([weights[:min(int(n), k)].sum() for n in answer_count]).to(dcg.device)\n",
    "       ndcg = (dcg / idcg).mean()\n",
    "       metrics['NDCG@%d' % k] = ndcg.cpu().item()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab88a3-a4e0-4554-a86c-3b50be88fc20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcea9f-babe-4150-b9b6-f9f8914a9321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08ca5f-1c12-4622-990b-195ff0c9ca54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521aa359-26ba-418b-a751-87388011f689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61c779ca-d0ac-4bf9-ad68-2b62b286961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative samples path: random_negative_samplesize0_seed12345.pkl\n",
      "Negative samples don't exist. Generating.\n",
      "Sampling negative items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 6040/6040 [00:00<00:00, 72782.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative samples path: random_negative_samplesize100_seed12345.pkl\n",
      "Negative samples don't exist. Generating.\n",
      "Sampling negative items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 6040/6040 [00:19<00:00, 311.96it/s]\n"
     ]
    }
   ],
   "source": [
    "df, umap, smap, train, val, test = load_data(args.data_path, args.min_rating, args.min_sc, args.min_uc)\n",
    "\n",
    "args.num_items = len(smap)\n",
    "\n",
    "\n",
    "dataloader = BertDataloader(args, umap, smap, train, val, test, './')\n",
    "train_dataloader, val_dataloader, test_dataloader = dataloader.get_pytorch_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b478a21e-eff2-4215-a70a-80191288806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTModel(args)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.decay_step, gamma=args.gamma)\n",
    "ce = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d1207ab-03a1-440e-becb-507f5415b0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of BERTModel(\n",
       "  (embedding): BERTEmbedding(\n",
       "    (token_embed): Embedding(3708, 256, padding_idx=0)\n",
       "    (position_embed): Embedding(100, 256, padding_idx=0)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-1): 2 x TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=256, out_features=3707, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd1cd0-9e75-445c-87e1-ac8b338aec31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d141b1-77b6-4bc9-a64f-7b78966e368f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1367dd-e5ab-4163-aa55-9a411da25715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(mode):\n",
    "    model.eval()\n",
    "\n",
    "    average_meter_set = AverageMeterSet()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if mode=='val':\n",
    "            tqdm_dataloader = tqdm(val_dataloader)\n",
    "        else:\n",
    "            tqdm_dataloader = tqdm(test_dataloader)\n",
    "            \n",
    "        for batch_idx, batch in enumerate(tqdm_dataloader):\n",
    "            batch = [x.to(args.device) for x in batch]\n",
    "            metrics = calculate_metrics(batch)\n",
    "\n",
    "            for k, v in metrics.items():\n",
    "                average_meter_set.update(k, v)\n",
    "            description_metrics = ['NDCG@%d' % k for k in args.metric_ks[:3]] +\\\n",
    "                                  ['Recall@%d' % k for k in args.metric_ks[:3]]\n",
    "            description = 'Val: ' + ', '.join(s + ' {:.3f}' for s in description_metrics)\n",
    "            description = description.format(*(average_meter_set[k].avg for k in description_metrics))\n",
    "            tqdm_dataloader.set_description(description)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19213c0d-5aba-4470-b5e7-892268403b5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch( epoch, accum_iter):\n",
    "\n",
    "    model.train()\n",
    "    lr_scheduler.step()\n",
    "    loss_history = deque(maxlen=200)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(train_dataloader, dynamic_ncols=True) ):\n",
    "        batch_size = batch[0].size(0)\n",
    "        batch = [x.to(args.device) for x in batch]\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss = calculate_loss(batch)\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "    \n",
    "        loss_history.append(loss.tolist())\n",
    "\n",
    "    torch.save(model, \"model_checkpoint_epoch_{}.pt\".format( str(epoch).zfill(2) ))\n",
    "    print(\"Epoch: \", epoch, \", Loss:\", np.mean(loss_history))\n",
    "\n",
    "    return accum_iter\n",
    "\n",
    "\n",
    "accum_iter = 0\n",
    "run_eval(mode='val')\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    accum_iter = train_one_epoch(epoch, accum_iter)\n",
    "    run_eval(mode='val')\n",
    "\n",
    "run_eval(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfb1c5-d21d-4ebd-ac59-47b5470d5f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b9242a-e281-4d45-b80a-e2dcafc77704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9525a-5bcb-4a4b-aef3-6ec867c85032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3853c-041b-4657-9764-e9254defd209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82696b91-a86a-4b0c-8558-cecf61a103e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f55ec7-af94-4176-9ec4-954e1d861dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ab900-7d6b-476a-a2b2-ddb90335b120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545bc86-0eb4-4905-940f-b850ceaa2951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
